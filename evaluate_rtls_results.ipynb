{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc23e5e9-e4c0-4f61-996b-1e00f4f6ac9a",
   "metadata": {},
   "source": [
    "# Evaluation Notebook for RTLS Results\n",
    "\n",
    "**Introduction:**\n",
    "This notebook is designed for the evaluation of multi-camera tracking results derived from the Real-Time Location System (RTLS) in its operational mode. It focuses on providing a comprehensive analysis of the RTLS accuracy by incorporating a variety of metrics, including the Higher Order Tracking Accuracy (HOTA), detection accuracy (DetA), association accuracy (AssA), localization accuracy (LocA), IDF1, MOTA, among other Multiple Object Tracking (MOT) metrics. These metrics collectively offer an in-depth perspective on the RTLS's overall effectiveness.\n",
    "\n",
    "RTLS technology is capable of generating 3D location data for all objects within a scene, projecting their positions onto the ground plane. This notebook serves as a tool for assessing the precision of these 3D locations by comparing them to ground-truth data, employing the Euclidean distance as the measure of accuracy. To utilize this notebook effectively, access to ground-truth 3D location data is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a283c3-1cec-4729-bdc6-20877b754b52",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "\n",
    "\n",
    "+ Ground truth file with 3D locations in MOT format\n",
    "+ RTLS log file produced from the `mdx-rtls` Kafka topic\n",
    "+ Valid config file (`app_rtls_config.json`) and calibration JSON file (`calibration.json`)\n",
    "\n",
    "\n",
    "**Environment Setup:** \n",
    "\n",
    "+ ``conda create -n mtmc_analytics python=3.10``\n",
    "+ ``conda activate mtmc_analytics``\n",
    "+ ``conda install jupyter notebook``\n",
    "+ ``pip3 install -r requirements.txt``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf16ff9",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab29355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Copyright (c) 2009-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.**\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Set, Tuple, Any\n",
    "\n",
    "import mdx.mtmc.utils.trackeval as trackeval\n",
    "from mdx.mtmc.config import AppConfig\n",
    "from mdx.mtmc.core.calibration import Calibrator\n",
    "from mdx.mtmc.utils.io_utils import validate_file_path, make_seq_ini_file, make_dir, load_json_from_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e24ed",
   "metadata": {},
   "source": [
    "# Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0380ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path: str = \"\"\n",
    "rtls_log_path: str = \"\"\n",
    "app_config_path: str = \"resources/app_rtls_config.json\"\n",
    "calibration_path: str = \"resources/calibration_retail_synthetic.json\"\n",
    "seq_length: int = 11000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98007bd5",
   "metadata": {},
   "source": [
    "# Define helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8221d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ground_truth_file(input_file_path: str, output_file_path: str, frame_ids: Set[int], ground_truth_frame_id_offset: int) -> None:\n",
    "    \"\"\"\n",
    "    Converts the ground truth file in a MOT file format for evaluation\n",
    "    \n",
    "    :param str input_file_path: input file path\n",
    "    :param str output_file_path: output file path\n",
    "    :param Set[int] frame_ids: frame IDs\n",
    "    :param int ground_truth_frame_id_offset: Offset of frame IDs in ground truth\n",
    "    :return: None\n",
    "    ::\n",
    "\n",
    "        prepare_ground_truth_file(input_file_path, output_file_path, frame_ids, ground_truth_frame_id_offset)\n",
    "    \"\"\"\n",
    "    # Create intermediate dictionary to store ground truth entry\n",
    "    map_frame_id_to_objects: Dict[int, Dict[int, List[float]]] = dict()\n",
    "\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            line_split = line.split(\" \")\n",
    "\n",
    "            # Extract frame ID from the line\n",
    "            frame_id = int(line_split[2]) + ground_truth_frame_id_offset\n",
    "\n",
    "            # Ignore the line if the frame ID is not present in the given frame IDs\n",
    "            if frame_id not in frame_ids:\n",
    "                continue\n",
    "\n",
    "            # Extract 3D location\n",
    "            location_x = float(line_split[7])\n",
    "            location_y = float(line_split[8])\n",
    "\n",
    "            # Append 3D location to the intermediate dictionary\n",
    "            if frame_id not in map_frame_id_to_objects:\n",
    "                map_frame_id_to_objects[frame_id] = dict()\n",
    "\n",
    "            # Extract object ID\n",
    "            object_id = int(line_split[1])\n",
    "            if object_id not in map_frame_id_to_objects[frame_id]:\n",
    "                map_frame_id_to_objects[frame_id][object_id] = list()\n",
    "            map_frame_id_to_objects[frame_id][object_id].append(location_x)\n",
    "            map_frame_id_to_objects[frame_id][object_id].append(location_y)\n",
    "\n",
    "    # Write the ground truth to output file\n",
    "    for frame_id in sorted(list(map_frame_id_to_objects.keys())):\n",
    "        for object_id in sorted(list(map_frame_id_to_objects[frame_id].keys())):\n",
    "            location_x = map_frame_id_to_objects[frame_id][object_id][0]\n",
    "            location_y = map_frame_id_to_objects[frame_id][object_id][1]\n",
    "            result_str = str(frame_id) + \" \" + str(object_id) + \" -1 -1 -1 -1 1 \" + str(location_x) + \" \" + str(location_y) + \" -1\\n\"\n",
    "            output_file.write(result_str)\n",
    "    del map_frame_id_to_objects\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "def prepare_prediction_file(input_file_path: str, output_file_path: str, ground_truth_delay_sec: float, fps: float) -> List[int]:\n",
    "    \"\"\"\n",
    "    Converts the predicition file in a MOT file format for evaluation\n",
    "    \n",
    "    :param str input_file_path: input file path\n",
    "    :param str output_file_path: output file path\n",
    "    :param float ground_truth_delay_sec: delay of ground truth in second\n",
    "    :param float fps: frame rate (FPS)\n",
    "    :return: list of frame IDs\n",
    "    :rtype: List[int]\n",
    "    ::\n",
    "\n",
    "        frame_ids = prepare_prediction_file(input_file_path, output_file_path, ground_truth_delay_sec, fps)\n",
    "    \"\"\"\n",
    "    # Create a intermediate dictionary to store the prediction entry\n",
    "    map_frame_id_to_objects: Dict[int, Dict[int, List[float]]] = dict()\n",
    "\n",
    "    # Read the input file that contains messages from the Kafka topic of \"mdx-rtls\"\n",
    "    with open(input_file_path) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "\n",
    "            # Remove the \"b\" symbol (byte) to read the line correctly \n",
    "            if line.startswith(\"b'\"):\n",
    "                line = line[2:-1]\n",
    "\n",
    "            # Load the line\n",
    "            line = json.loads(line)\n",
    "\n",
    "            # Rectify the frame ID based on the delay of ground truth\n",
    "            raw_frame_id = int(line[\"frameId\"])\n",
    "            corrected_frame_id = int(raw_frame_id - ((ground_truth_delay_sec / 2) * fps))\n",
    "            if corrected_frame_id <= 0:\n",
    "                continue\n",
    "\n",
    "            # Read the 3D locations of objects\n",
    "            for object_info in line[\"locationsOfObjects\"]:\n",
    "                object_info = str(object_info)\n",
    "                object_info = object_info.replace(\"'\", '\"')\n",
    "                object_json = json.loads(object_info)\n",
    "\n",
    "                # Ignore log if location is not present\n",
    "                if len(object_json[\"locations\"]) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Extract 3D location\n",
    "                location_x = object_json[\"locations\"][0][0]\n",
    "                location_y = object_json[\"locations\"][0][1]\n",
    "\n",
    "                if corrected_frame_id not in map_frame_id_to_objects:\n",
    "                    map_frame_id_to_objects[corrected_frame_id] = dict()\n",
    "\n",
    "                # Extract the object ID\n",
    "                object_id = int(object_json[\"id\"])  \n",
    "                if object_id not in map_frame_id_to_objects[corrected_frame_id]:\n",
    "                    map_frame_id_to_objects[corrected_frame_id][object_id] = list()\n",
    "                map_frame_id_to_objects[corrected_frame_id][object_id].append(location_x)\n",
    "                map_frame_id_to_objects[corrected_frame_id][object_id].append(location_y)\n",
    "\n",
    "    output_file = open(output_file_path, \"w\")\n",
    "\n",
    "    # Write the prediction to output file\n",
    "    for frame_id in sorted(list(map_frame_id_to_objects.keys())):\n",
    "        for object_id in sorted(list(map_frame_id_to_objects[frame_id].keys())):\n",
    "            location_x = map_frame_id_to_objects[frame_id][object_id][0]\n",
    "            location_y = map_frame_id_to_objects[frame_id][object_id][1]\n",
    "            result_str = str(frame_id) + \" \" + str(object_id) + \" -1 -1 -1 -1 1 \" + str(location_x) + \" \" + str(location_y) + \" -1\\n\"\n",
    "            output_file.write(result_str)\n",
    "\n",
    "    output_file.close()\n",
    "    frame_ids = list(map_frame_id_to_objects.keys())\n",
    "    return frame_ids\n",
    "\n",
    "\n",
    "def make_seq_maps_file(seq_maps_dir_path: str, sensor_ids: List[str], benchmark: str, split_to_eval: str) -> None:\n",
    "    \"\"\"\n",
    "    Makes a sequence-maps file used by TrackEval library\n",
    "\n",
    "    :param str seq_maps_dir_path: output directory path\n",
    "    :param List[str] sensor_ids: sensor IDs\n",
    "    :param str benchmark: name of the benchmark\n",
    "    :param str split_to_eval: name of the split for evaluation\n",
    "    :return: None\n",
    "    ::\n",
    "\n",
    "        make_seq_maps_file(seq_maps_dir_path, sensor_ids, benchmark, split_to_eval)\n",
    "    \"\"\"\n",
    "    make_dir(seq_maps_dir_path)\n",
    "    seq_maps_file_name = benchmark + \"-\" + split_to_eval + \".txt\"\n",
    "    seq_maps_file_path = os.path.join(seq_maps_dir_path, seq_maps_file_name)\n",
    "    f = open(seq_maps_file_path, \"w\")\n",
    "    f.write(\"name\\n\")\n",
    "\n",
    "    for sensor_id in sensor_ids:\n",
    "        f.write(sensor_id + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def setup_evaluation_configs(results_dir_path: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Sets up evaluation configurations\n",
    "\n",
    "    :param str results_dir_path: path to the folder that stores the results\n",
    "    :return: dataset configuration and evaluation configuration\n",
    "    :rtype: Tuple[Dict[str,Any],Dict[str,Any]]\n",
    "    ::\n",
    "\n",
    "        dataset_config, eval_config = setup_evaluation_configs(results_dir_path)\n",
    "    \"\"\"\n",
    "    eval_config = trackeval.eval.Evaluator.get_default_eval_config()\n",
    "    eval_config[\"PRINT_CONFIG\"] = False\n",
    "    eval_config[\"USE_PARALLEL\"] = True\n",
    "    \n",
    "    # Create dataset configs for TrackEval library\n",
    "    dataset_config = trackeval.datasets.MotChallenge3DLocation.get_default_dataset_config()\n",
    "    dataset_config[\"DO_PREPROC\"] = False\n",
    "    dataset_config[\"SPLIT_TO_EVAL\"] = \"all\"\n",
    "    evaluation_dir_path = os.path.join(results_dir_path, \"evaluation\")\n",
    "    make_dir(evaluation_dir_path)\n",
    "    dataset_config[\"GT_FOLDER\"] = os.path.join(evaluation_dir_path, \"gt\")\n",
    "    dataset_config[\"TRACKERS_FOLDER\"] = os.path.join(evaluation_dir_path, \"scores\")\n",
    "    dataset_config[\"PRINT_CONFIG\"] = False\n",
    "\n",
    "    return dataset_config, eval_config\n",
    "\n",
    "\n",
    "def prepare_evaluation_folder(dataset_config: Dict[str, Any]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Prepares evaluation folder\n",
    "\n",
    "    :param Dict[str,Any] dataset_config: dataset configuration\n",
    "    :return: prediction file path and ground truth file path\n",
    "    :rtype: Tuple[str,str]\n",
    "    ::\n",
    "\n",
    "        pred_file_path, gt_file_path = prepare_evaluation_folder(dataset_config)\n",
    "    \"\"\"\n",
    "    # Create evaluation configs for TrackEval library\n",
    "    sensor_ids: Set[str] = set()\n",
    "    sensor_ids.add(\"RTLS\")\n",
    "    sensor_ids = sorted(list(sensor_ids))\n",
    "\n",
    "    # Create sequence maps file for evaluation\n",
    "    seq_maps_dir_path = os.path.join(dataset_config[\"GT_FOLDER\"], \"seqmaps\")\n",
    "    make_seq_maps_file(seq_maps_dir_path, sensor_ids, dataset_config[\"BENCHMARK\"], dataset_config[\"SPLIT_TO_EVAL\"])\n",
    "\n",
    "    # Create ground truth directory\n",
    "    mot_version = dataset_config[\"BENCHMARK\"] + \"-\" + dataset_config[\"SPLIT_TO_EVAL\"]\n",
    "    gt_root_dir_path = os.path.join(dataset_config[\"GT_FOLDER\"], mot_version)\n",
    "    gt_rtls_dir_path = os.path.join(gt_root_dir_path, \"RTLS\")\n",
    "    make_dir(gt_rtls_dir_path)\n",
    "    gt_output_dir_path = os.path.join(gt_rtls_dir_path, \"gt\")\n",
    "    make_dir(gt_output_dir_path)\n",
    "    gt_file_path = os.path.join(gt_output_dir_path, \"gt.txt\")\n",
    "\n",
    "    # Generate sequence file required for TrackEval library\n",
    "    make_seq_ini_file(gt_rtls_dir_path, camera=\"RTLS\", seq_length=seq_length)          \n",
    "\n",
    "    # Create prediction directory\n",
    "    pred_dir_path = os.path.join(dataset_config[\"TRACKERS_FOLDER\"], mot_version, \"data\", \"data\")\n",
    "    make_dir(pred_dir_path)\n",
    "    pred_file_path = os.path.join(pred_dir_path, \"RTLS.txt\")\n",
    "\n",
    "    return pred_file_path, gt_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ffb00f",
   "metadata": {},
   "source": [
    "# Start evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba17a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load app config\n",
    "app_config = AppConfig(**load_json_from_file(app_config_path))\n",
    "\n",
    "# Prepare evaluation folders\n",
    "dataset_config, eval_config = setup_evaluation_configs(app_config.io.outputDirPath)\n",
    "pred_file_path, gt_file_path = prepare_evaluation_folder(dataset_config)\n",
    "\n",
    "# Get the MTMC plus config parameters\n",
    "ground_truth_frame_id_offset = app_config.io.groundTruthFrameIdOffset\n",
    "mtmc_plus_location_window_sec = app_config.streaming.mtmcPlusLocationWindowSec\n",
    "mtmc_plus_smoothing_window_sec = app_config.streaming.mtmcPlusSmoothingWindowSec\n",
    "\n",
    "# Get FPS for sensor\n",
    "calibrator = Calibrator(calibration_path)\n",
    "sensors = calibrator.load_calibration_file(calibration_path)\n",
    "fps = None\n",
    "for sensor in sensors:\n",
    "    for attribute in sensor.attributes:\n",
    "        if attribute[\"name\"] == \"fps\":\n",
    "            if fps is None:\n",
    "                fps = float(attribute[\"value\"])\n",
    "            elif fps != float(attribute[\"value\"]):\n",
    "                print(f\"ERROR: Unmatched FPS for sensors: {fps} != {float(attribute['value'])}.\")\n",
    "                exit(1)\n",
    "if fps is None:\n",
    "    print(f\"ERROR: FPS not available in calibration.\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Found mtmcPlusLocationWindowSec as: {mtmc_plus_location_window_sec} seconds\")\n",
    "print(f\"Found mtmcPlusSmoothingWindowSec as: {mtmc_plus_smoothing_window_sec} seconds\")\n",
    "print(f\"Found sensor FPS as: {fps}\")\n",
    "\n",
    "# Generate prediction file\n",
    "frame_ids = prepare_prediction_file(rtls_log_path, pred_file_path, (mtmc_plus_location_window_sec + mtmc_plus_smoothing_window_sec), fps)\n",
    "\n",
    "# Generate ground truth file \n",
    "prepare_ground_truth_file(ground_truth_path, gt_file_path, set(frame_ids), ground_truth_frame_id_offset)\n",
    "\n",
    "# Define the metrics to calculate\n",
    "metrics_config = {\"METRICS\": [\"HOTA\", \"CLEAR\", \"Identity\"]}\n",
    "metrics_config[\"PRINT_CONFIG\"] = False\n",
    "config = {**eval_config, **dataset_config, **metrics_config}  # Merge configs\n",
    "eval_config = {k: v for k, v in config.items() if k in eval_config.keys()}\n",
    "dataset_config = {k: v for k, v in config.items() if k in dataset_config.keys()}\n",
    "metrics_config = {k: v for k, v in config.items() if k in metrics_config.keys()}\n",
    "\n",
    "# Run the Evaluator\n",
    "evaluator = trackeval.eval.Evaluator(eval_config)\n",
    "dataset_list = [trackeval.datasets.MotChallenge3DLocation(dataset_config)]\n",
    "metrics_list: List[str] = list()\n",
    "for metric in [trackeval.metrics.HOTA, trackeval.metrics.CLEAR, trackeval.metrics.Identity]:\n",
    "    if metric.get_name() in metrics_config[\"METRICS\"]:\n",
    "        metrics_list.append(metric(metrics_config))\n",
    "if len(metrics_list) == 0:\n",
    "    raise Exception(\"No metric selected for evaluation.\")\n",
    "evaluator.evaluate(dataset_list, metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578439b9-8b3b-4064-8471-0ec48a02cf16",
   "metadata": {},
   "source": [
    "# Plot evaluation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_dir_path = os.path.join(app_config.io.outputDirPath, \"evaluation\", \"plots\")\n",
    "make_dir(plots_dir_path)\n",
    "trackers_dir_path = dataset_config[\"TRACKERS_FOLDER\"]\n",
    "classes: List[str] = [\"pedestrian\"]\n",
    "mot_version = dataset_config[\"BENCHMARK\"] + \"-\" + dataset_config[\"SPLIT_TO_EVAL\"]\n",
    "data_dir_path = os.path.join(trackers_dir_path, mot_version)\n",
    "tracker_names = os.listdir(validate_file_path(data_dir_path))\n",
    "for cls in classes:\n",
    "    trackeval.plotting.plot_compare_trackers(data_dir_path, tracker_names, cls, plots_dir_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f71278-10c9-4f19-ab06-8b0751d9503c",
   "metadata": {},
   "source": [
    "# Delete output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5065d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {app_config.io.outputDirPath}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
